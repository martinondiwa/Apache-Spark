{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYUvtf9phbIi"
      },
      "outputs": [],
      "source": [
        "# STEP 1 ############################################################\n",
        "# GOAL:\n",
        "#   Install PySpark in Google Colab.\n",
        "#\n",
        "# WHAT IS HAPPENING:\n",
        "#   - Apache Spark is a distributed data processing engine (NOT a database).\n",
        "#   - In Colab, we do NOT manually install Java or download Spark binaries.\n",
        "#   - Installing `pyspark` is enough to use Spark from Python in Colab.\n",
        "#\n",
        "# EXPECTED RESULT:\n",
        "#   - This finishes quietly (no output or minimal output).\n",
        "#   - If you re-run it, it’s fine.\n",
        "############################################################\n",
        "\n",
        "!pip install -q pyspark\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2 ############################################################\n",
        "# GOAL:\n",
        "#   Start a SparkSession (the main entry point to Spark).\n",
        "#\n",
        "# WHAT IS SPARKSESSION:\n",
        "#   - Think of SparkSession as the \"Spark engine handle\".\n",
        "#   - You use it to create DataFrames, read files, and run SQL queries.\n",
        "#\n",
        "# EXPECTED RESULT:\n",
        "#   - You should see Spark version printed.\n",
        "#   - `spark.sparkContext.master` will usually show \"local[*]\" in Colab,\n",
        "#     meaning Spark runs locally using available CPU cores in the Colab VM.\n",
        "############################################################\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "    .appName(\"CIS660_Spark_Tutorial\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "print(\"Spark version:\", spark.version)\n",
        "print(\"Spark master:\", spark.sparkContext.master)\n"
      ],
      "metadata": {
        "id": "mN6_jqd5hgcf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abfcc010-8f47-48f0-b121-13f08218b586"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark version: 4.0.2\n",
            "Spark master: local[*]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3 ############################################################\n",
        "# GOAL:\n",
        "#   Create your first Spark DataFrame (a distributed table).\n",
        "#\n",
        "# WHAT IS A DATAFRAME IN SPARK:\n",
        "#   - Similar idea to a pandas DataFrame or an SQL table.\n",
        "#   - But Spark DataFrames are designed to scale across many machines.\n",
        "#   - Even in Colab (single machine), you use the same API.\n",
        "#\n",
        "# EXPECTED RESULT:\n",
        "#   - `show()` prints a preview of rows (like a table).\n",
        "#   - `printSchema()` shows Spark’s inferred data types.\n",
        "############################################################\n",
        "\n",
        "data = [\n",
        "    (\"Alice\", \"Sales\", 70000),\n",
        "    (\"Bob\", \"Sales\", 60000),\n",
        "    (\"Charlie\", \"Engineering\", 120000),\n",
        "]\n",
        "\n",
        "columns = [\"name\", \"department\", \"salary\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "df.show()\n",
        "df.printSchema()\n"
      ],
      "metadata": {
        "id": "JFe8B6zUhrsG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "832fd90a-e3b2-439b-a678-815d3ab3273e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+------+\n",
            "|   name| department|salary|\n",
            "+-------+-----------+------+\n",
            "|  Alice|      Sales| 70000|\n",
            "|    Bob|      Sales| 60000|\n",
            "|Charlie|Engineering|120000|\n",
            "+-------+-----------+------+\n",
            "\n",
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- department: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 4 ############################################################\n",
        "# GOAL:\n",
        "#   Perform basic DataFrame operations: select, filter, and sort.\n",
        "#\n",
        "# WHY THIS MATTERS:\n",
        "#   - These are the core operations you’ll use in every Spark project.\n",
        "#   - They mirror SQL concepts:\n",
        "#       SELECT columns\n",
        "#       WHERE filters\n",
        "#       ORDER BY sorting\n",
        "#\n",
        "# NOTE:\n",
        "#   - Many Spark operations are \"transformations\" (they build a plan).\n",
        "#   - Results are only computed when you run an \"action\" (e.g., show, count).\n",
        "############################################################\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# SELECT-like behavior: choose specific columns\n",
        "df.select(\"name\", \"salary\").show()\n",
        "\n",
        "# WHERE-like behavior: filter rows\n",
        "df.filter(col(\"salary\") > 65000).show()\n",
        "\n",
        "# ORDER BY-like behavior: sort rows\n",
        "df.orderBy(col(\"salary\").desc()).show()\n"
      ],
      "metadata": {
        "id": "8WDfvAdSh1eV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93ad3445-b32a-4140-baa3-edaa6715fcc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+\n",
            "|   name|salary|\n",
            "+-------+------+\n",
            "|  Alice| 70000|\n",
            "|    Bob| 60000|\n",
            "|Charlie|120000|\n",
            "+-------+------+\n",
            "\n",
            "+-------+-----------+------+\n",
            "|   name| department|salary|\n",
            "+-------+-----------+------+\n",
            "|  Alice|      Sales| 70000|\n",
            "|Charlie|Engineering|120000|\n",
            "+-------+-----------+------+\n",
            "\n",
            "+-------+-----------+------+\n",
            "|   name| department|salary|\n",
            "+-------+-----------+------+\n",
            "|Charlie|Engineering|120000|\n",
            "|  Alice|      Sales| 70000|\n",
            "|    Bob|      Sales| 60000|\n",
            "+-------+-----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 5 ############################################################\n",
        "# GOAL:\n",
        "#   Group and aggregate data (e.g., average salary per department).\n",
        "#\n",
        "# WHY THIS MATTERS:\n",
        "#   - Aggregations are where Spark becomes very powerful at scale.\n",
        "#   - groupBy + aggregations is a standard analytics workflow.\n",
        "#\n",
        "# EXPECTED RESULT:\n",
        "#   - One row per department with count and average salary.\n",
        "############################################################\n",
        "\n",
        "from pyspark.sql.functions import avg, count\n",
        "\n",
        "(\n",
        "    df.groupBy(\"department\")\n",
        "      .agg(\n",
        "          count(\"*\").alias(\"num_people\"),\n",
        "          avg(\"salary\").alias(\"avg_salary\")\n",
        "      )\n",
        "      .show()\n",
        ")\n"
      ],
      "metadata": {
        "id": "6BA4ltsqh66-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "879c8aa8-89d7-4cd1-eff6-c6d0881d1809"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+----------+\n",
            "| department|num_people|avg_salary|\n",
            "+-----------+----------+----------+\n",
            "|      Sales|         2|   65000.0|\n",
            "|Engineering|         1|  120000.0|\n",
            "+-----------+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 6 ############################################################\n",
        "# GOAL:\n",
        "#   Understand \"lazy evaluation\" (transformations vs actions).\n",
        "#\n",
        "# KEY IDEA:\n",
        "#   - Spark does NOT immediately run transformations.\n",
        "#   - It builds an execution plan (DAG).\n",
        "#   - An ACTION triggers execution.\n",
        "#\n",
        "# WHAT WE DO:\n",
        "#   - Create a transformation `plan`\n",
        "#   - Use `explain()` to view Spark’s planned execution\n",
        "#   - Use `show()` (an action) to trigger the computation\n",
        "############################################################\n",
        "\n",
        "plan = df.filter(col(\"salary\") > 60000).select(\"name\", \"department\")\n",
        "\n",
        "print(\"This is a DataFrame object (a plan), not computed results:\")\n",
        "print(plan)\n",
        "\n",
        "print(\"\\nSpark execution plan:\")\n",
        "plan.explain(True)\n",
        "\n",
        "print(\"\\nAction: show() triggers execution:\")\n",
        "plan.show()\n"
      ],
      "metadata": {
        "id": "1UMG1I67h_my",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "683890db-4f64-48fb-9161-1633158b922c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a DataFrame object (a plan), not computed results:\n",
            "DataFrame[name: string, department: string]\n",
            "\n",
            "Spark execution plan:\n",
            "== Parsed Logical Plan ==\n",
            "'Project ['name, 'department]\n",
            "+- Filter (salary#2L > cast(60000 as bigint))\n",
            "   +- LogicalRDD [name#0, department#1, salary#2L], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "name: string, department: string\n",
            "Project [name#0, department#1]\n",
            "+- Filter (salary#2L > cast(60000 as bigint))\n",
            "   +- LogicalRDD [name#0, department#1, salary#2L], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [name#0, department#1]\n",
            "+- Filter (isnotnull(salary#2L) AND (salary#2L > 60000))\n",
            "   +- LogicalRDD [name#0, department#1, salary#2L], false\n",
            "\n",
            "== Physical Plan ==\n",
            "*(1) Project [name#0, department#1]\n",
            "+- *(1) Filter (isnotnull(salary#2L) AND (salary#2L > 60000))\n",
            "   +- *(1) Scan ExistingRDD[name#0,department#1,salary#2L]\n",
            "\n",
            "\n",
            "Action: show() triggers execution:\n",
            "+-------+-----------+\n",
            "|   name| department|\n",
            "+-------+-----------+\n",
            "|  Alice|      Sales|\n",
            "|Charlie|Engineering|\n",
            "+-------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 7 ############################################################\n",
        "# GOAL:\n",
        "#   Read data from a CSV file using Spark.\n",
        "#\n",
        "# IMPORTANT FOR STUDENTS:\n",
        "#   Later in labs you will load real datasets from Google Drive.\n",
        "#   To do that:\n",
        "#     1) Mount Google Drive (see STEP 14)\n",
        "#     2) In the Colab left sidebar → Files → Google Drive\n",
        "#     3) Right-click your file → Copy path\n",
        "#     4) Paste the path into spark.read.csv(...)\n",
        "#\n",
        "# WHY WE CREATE A SMALL CSV FIRST:\n",
        "#   - Keeps this tutorial self-contained.\n",
        "#   - Does NOT use lab datasets.\n",
        "############################################################\n",
        "\n",
        "csv_text = \"\"\"student_id,name,course,score,credits,tags\n",
        "1,Alice,Math,85,3,hard|core\n",
        "2,Bob,Math,90,3,important|exam\n",
        "3,Charlie,Physics,78,4,lab|science\n",
        "4,Diana,Physics,88,4,science|advanced\n",
        "5,Evan,History,92,3,reading|essay\n",
        "\"\"\"\n",
        "\n",
        "csv_path = \"/content/students_sample.csv\"\n",
        "\n",
        "with open(csv_path, \"w\") as f:\n",
        "    f.write(csv_text)\n",
        "\n",
        "df_students = (\n",
        "    spark.read\n",
        "    .option(\"header\", True)\n",
        "    .option(\"inferSchema\", True)\n",
        "    .csv(csv_path)\n",
        ")\n",
        "\n",
        "df_students.printSchema()\n",
        "df_students.show(truncate=False)\n"
      ],
      "metadata": {
        "id": "-82Lgo40iKN4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df914764-56a8-4b6b-b207-797b99139580"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- student_id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- course: string (nullable = true)\n",
            " |-- score: integer (nullable = true)\n",
            " |-- credits: integer (nullable = true)\n",
            " |-- tags: string (nullable = true)\n",
            "\n",
            "+----------+-------+-------+-----+-------+----------------+\n",
            "|student_id|name   |course |score|credits|tags            |\n",
            "+----------+-------+-------+-----+-------+----------------+\n",
            "|1         |Alice  |Math   |85   |3      |hard|core       |\n",
            "|2         |Bob    |Math   |90   |3      |important|exam  |\n",
            "|3         |Charlie|Physics|78   |4      |lab|science     |\n",
            "|4         |Diana  |Physics|88   |4      |science|advanced|\n",
            "|5         |Evan   |History|92   |3      |reading|essay   |\n",
            "+----------+-------+-------+-----+-------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 8 ############################################################\n",
        "# GOAL:\n",
        "#   Demonstrate safe type conversion (data cleaning).\n",
        "#\n",
        "# WHY THIS MATTERS:\n",
        "#   Real datasets may contain invalid numeric values.\n",
        "#   We protect casting using regex validation.\n",
        "############################################################\n",
        "\n",
        "from pyspark.sql.functions import trim, when\n",
        "\n",
        "df_clean = (\n",
        "    df_students\n",
        "    .withColumn(\n",
        "        \"score_num\",\n",
        "        when(trim(col(\"score\")).rlike(r\"^[0-9]+$\"),\n",
        "             trim(col(\"score\")).cast(\"int\"))\n",
        "    )\n",
        "    .withColumn(\n",
        "        \"credits_num\",\n",
        "        when(trim(col(\"credits\")).rlike(r\"^[0-9]+$\"),\n",
        "             trim(col(\"credits\")).cast(\"int\"))\n",
        "    )\n",
        ")\n",
        "\n",
        "df_clean.show()\n"
      ],
      "metadata": {
        "id": "S_VeJtLYicpF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0413241-02ea-46c5-ba4c-8924acb567fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------+-------+-----+-------+----------------+---------+-----------+\n",
            "|student_id|   name| course|score|credits|            tags|score_num|credits_num|\n",
            "+----------+-------+-------+-----+-------+----------------+---------+-----------+\n",
            "|         1|  Alice|   Math|   85|      3|       hard|core|       85|          3|\n",
            "|         2|    Bob|   Math|   90|      3|  important|exam|       90|          3|\n",
            "|         3|Charlie|Physics|   78|      4|     lab|science|       78|          4|\n",
            "|         4|  Diana|Physics|   88|      4|science|advanced|       88|          4|\n",
            "|         5|   Evan|History|   92|      3|   reading|essay|       92|          3|\n",
            "+----------+-------+-------+-----+-------+----------------+---------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 9 ############################################################\n",
        "# GOAL:\n",
        "#   Aggregate student performance by course.\n",
        "#\n",
        "# WHY:\n",
        "#   groupBy + aggregation is a core Spark workflow.\n",
        "############################################################\n",
        "\n",
        "from pyspark.sql.functions import avg, count\n",
        "\n",
        "(\n",
        "    df_clean.groupBy(\"course\")\n",
        "    .agg(\n",
        "        count(\"*\").alias(\"num_students\"),\n",
        "        avg(\"score_num\").alias(\"avg_score\")\n",
        "    )\n",
        "    .show()\n",
        ")\n"
      ],
      "metadata": {
        "id": "W4dRamkWijLm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "924b0f28-0acb-4e97-adda-cc2a192faf57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------+---------+\n",
            "| course|num_students|avg_score|\n",
            "+-------+------------+---------+\n",
            "|   Math|           2|     87.5|\n",
            "|History|           1|     92.0|\n",
            "|Physics|           2|     83.0|\n",
            "+-------+------------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 10 ############################################################\n",
        "# GOAL:\n",
        "#   Use Spark SQL on a DataFrame.\n",
        "#\n",
        "# WHY THIS MATTERS:\n",
        "#   - Spark supports two interfaces:\n",
        "#       (1) DataFrame API (Python)\n",
        "#       (2) Spark SQL\n",
        "#   - SQL is often easier for students who already know SQL.\n",
        "#\n",
        "# WHAT WE DO:\n",
        "#   - Register a temporary view (like a temporary table)\n",
        "#   - Run a SQL query against it\n",
        "#\n",
        "# NOTE:\n",
        "#   - Temp views live only in the current Spark session.\n",
        "############################################################\n",
        "\n",
        "df_clean.createOrReplaceTempView(\"students\")\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "SELECT name, course, score_num\n",
        "FROM students\n",
        "WHERE score_num >= 85\n",
        "ORDER BY score_num DESC\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "id": "iOM988hGinhc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab1906aa-b495-439d-9f4f-a7c24408a6c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------+---------+\n",
            "| name| course|score_num|\n",
            "+-----+-------+---------+\n",
            "| Evan|History|       92|\n",
            "|  Bob|   Math|       90|\n",
            "|Diana|Physics|       88|\n",
            "|Alice|   Math|       85|\n",
            "+-----+-------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 11 ############################################################\n",
        "# GOAL:\n",
        "#   Learn explode() using Student example (tags separated by |).\n",
        "#\n",
        "# WHY THIS MATTERS:\n",
        "#   - explode() converts an array/list into multiple rows.\n",
        "#   - This is common in log data, tags, keywords, and text pipelines.\n",
        "#\n",
        "# WHAT WE DO:\n",
        "#   - Split tags on \"|\"\n",
        "#   - Explode into one row per tag\n",
        "\n",
        "\n",
        "from pyspark.sql.functions import split, explode\n",
        "\n",
        "df_tags = df_clean.withColumn(\n",
        "    \"tag\",\n",
        "    explode(split(col(\"tags\"), r\"\\|\"))\n",
        ")\n",
        "\n",
        "df_tags.select(\"name\", \"course\", \"tag\").show()\n"
      ],
      "metadata": {
        "id": "ibZoidyriunH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faafd0d7-6257-469c-85a9-1efc3cc283a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+---------+\n",
            "|   name| course|      tag|\n",
            "+-------+-------+---------+\n",
            "|  Alice|   Math|     hard|\n",
            "|  Alice|   Math|     core|\n",
            "|    Bob|   Math|important|\n",
            "|    Bob|   Math|     exam|\n",
            "|Charlie|Physics|      lab|\n",
            "|Charlie|Physics|  science|\n",
            "|  Diana|Physics|  science|\n",
            "|  Diana|Physics| advanced|\n",
            "|   Evan|History|  reading|\n",
            "|   Evan|History|    essay|\n",
            "+-------+-------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 12 ############################################################\n",
        "# GOAL:\n",
        "#   Count how often each tag appears.\n",
        "#\n",
        "# WHY:\n",
        "#   This demonstrates distributed counting.\n",
        "############################################################\n",
        "\n",
        "from pyspark.sql.functions import desc\n",
        "\n",
        "(\n",
        "    df_tags.groupBy(\"tag\")\n",
        "    .count()\n",
        "    .orderBy(desc(\"count\"))\n",
        "    .show()\n",
        ")\n"
      ],
      "metadata": {
        "id": "mxMpqe1DixsN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "032811b1-cfb0-44c5-d8cd-952db7ea222a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----+\n",
            "|      tag|count|\n",
            "+---------+-----+\n",
            "|  science|    2|\n",
            "|     exam|    1|\n",
            "|  reading|    1|\n",
            "|     core|    1|\n",
            "|      lab|    1|\n",
            "|    essay|    1|\n",
            "|     hard|    1|\n",
            "|important|    1|\n",
            "| advanced|    1|\n",
            "+---------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 13 ############################################################\n",
        "# GOAL:\n",
        "#   Write output results to disk.\n",
        "#\n",
        "# NOTE:\n",
        "#   Spark writes output as a folder (distributed files).\n",
        "############################################################\n",
        "\n",
        "out_path = \"/content/student_tag_counts\"\n",
        "\n",
        "(\n",
        "    df_tags.groupBy(\"tag\")\n",
        "    .count()\n",
        "    .coalesce(1)\n",
        "    .write\n",
        "    .mode(\"overwrite\")\n",
        "    .csv(out_path)\n",
        ")\n",
        "\n",
        "print(\"Saved results to:\", out_path)\n"
      ],
      "metadata": {
        "id": "fVyxdGJUi0JP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6534bc5f-3d12-49fe-b4bd-7241dcc5a840"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved results to: /content/student_tag_counts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 14 ############################################################\n",
        "# OPTIONAL: GOOGLE DRIVE FOR REAL DATASETS\n",
        "#\n",
        "# INSTRUCTIONS FOR STUDENTS:\n",
        "#   1) Uncomment drive.mount() and run this cell\n",
        "#   2) In the left sidebar → Files → Google Drive\n",
        "#   3) Find your dataset\n",
        "#   4) Right-click → Copy path\n",
        "#   5) Paste the path into spark.read.csv(...)\n",
        "############################################################\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Example usage:\n",
        "# path = \"/content/drive/MyDrive/YourFolder/your_dataset.csv\"\n",
        "# df_real = spark.read.option(\"header\", True).csv(path)\n",
        "# df_real.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "DMgDUQ6Wi_Qd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}